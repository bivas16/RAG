{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Standard RAG approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python==0.2.37 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (0.2.37)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from llama-cpp-python==0.2.37) (3.1.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from llama-cpp-python==0.2.37) (5.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from llama-cpp-python==0.2.37) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from llama-cpp-python==0.2.37) (1.22.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.37) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "! CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 /Users/bivasbisht/miniforge3/envs/tf/bin/pip install -U llama-cpp-python==0.2.37 --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "\n",
    "import langchain \n",
    "from langchain_community.llms import LlamaCpp\n",
    "# loaders\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# splits\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# prompts\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# vector stores\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# models\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# retrievers\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "print('LangChain:', langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # LLMs\n",
    "    model_name = 'llama2-7b'# llama2-7b, llama2-13b (gguf models that can run on cpu)\n",
    "    temperature = 0,\n",
    "    top_p = 0.95,\n",
    "    repetition_penalty = 1.15\n",
    "\n",
    "    # splitting\n",
    "    split_chunk_size = 800\n",
    "    split_overlap = 0\n",
    "\n",
    "    #overlap is set to 0, meaning there's no overlap between chunks. Each chunk starts exactly where the previous one ends.\n",
    "\n",
    "    # embeddings\n",
    "    embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "    # similar passages\n",
    "    k = 3\n",
    "\n",
    "    # paths\n",
    "    PDFs_path = 'File_data'\n",
    "    Embeddings_path = 'faiss_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/bivasbisht/Thesis/llama-2-7b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   140.56 MiB, (  486.36 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4560.87 MiB\n",
      "llm_load_tensors:      Metal buffer size =   140.56 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, (  518.61 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    12.01 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (  518.62 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   171.61 MiB, (  690.22 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   171.60 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   167.20 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '17', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 \n",
    "n_batch = 512  #number of tokens to process in parallel. \n",
    "\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/bivasbisht/Thesis/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 13.8 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10211.27 ms\n",
      "llama_print_timings:      sample time =      45.59 ms /   243 runs   (    0.19 ms per token,  5330.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10210.80 ms /     7 tokens ( 1458.69 ms per token,     0.69 tokens per second)\n",
      "llama_print_timings:        eval time = 2603727.02 ms /   242 runs   (10759.20 ms per token,     0.09 tokens per second)\n",
      "llama_print_timings:       total time = 2615854.47 ms /   249 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWho is the founder of Nzeb?\\nNzeb is a blockchain-based platform that leverages artificial intelligence (AI) and machine learning (ML) to provide a decentralized ecosystem for various industries. The platform aims to connect businesses, individuals, and machines in a secure and transparent manner, enabling them to collaborate and exchange value without intermediaries.\\nNzeb was founded by a team of experienced entrepreneurs and blockchain enthusiasts who saw the potential of blockchain technology to revolutionize various industries. The company's mission is to create a decentralized platform that empowers individuals, businesses, and machines to work together in a more efficient and secure manner, while also promoting transparency and trust.\\nNzeb's founder is not publicly disclosed on the company's website or other reputable sources. However, the team behind Nzeb consists of experienced professionals with backgrounds in blockchain development, AI, ML, and software development. They have extensive knowledge and expertise in building decentralized platforms and developing innovative solutions for various industries.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "llm.invoke(\"what is Nzeb?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and splitting of data (pdf file data) -- > 1 week\n",
    "# get embeddings done --\n",
    "# store in vector db\n",
    "# set up retriever\n",
    "# prompt code\n",
    "# run llm with custom data (conservation data)\n",
    "\n",
    "#----------\n",
    "# Second Approach:\n",
    "# directly using embedding model , which already has embeddings of the original data.\n",
    "\n",
    "#Data Loade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 25 µs\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "#following the second approach : loading the embedding model directly\n",
    "\n",
    "%time\n",
    "\n",
    "### download embeddings model\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name = Config.embeddings_model_repo\n",
    "    # model_kwargs = {\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "### load vector DB embeddings\n",
    "vectordb = FAISS.load_local(\n",
    "    Config.Embeddings_path,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='gained each year as a result of deep retrofitting of buildings, and the construction of Passivhaus1 \\n \\n1 Passivhaus residences are very low energy buildings which require little energy for space heating and cooling. In particular, Passivhaus buildings efficiently exploit sun, \\ninternal heat sources, heat recovery, strategic shading, passive cooling techniques, hence rendering conventional heating and cooling systems unnecessary.', metadata={'source': '/content/gdrive/MyDrive/thesis data/File_data/REFEREE Real Value of Energy Efficiency.pdf', 'page': 7}),\n",
       " Document(page_content='Journal of Building Engineering 76 (2023) 107234\\n3buildings [11]. \\nMember States are allowed to adapt the regulations to their specific needs with a progressive tightening of minimum requirements \\ntowards a nZEB model [12]. Several Member States have already provided a quantitative definition and chose the Passivhaus standard \\nas a reference to ensure that the energy demands of buildings in their territory align with those of a nZEB [13]. Passivhaus (PH) is a \\nwidely known German energy performance standard, which requires minimized energy demand for space heating and therefore, thick \\ninsulation, absence of thermal bridges, airtightness, efficient windows, and heat recovery ventilation [14]. A nZEB complies with the', metadata={'source': '/content/gdrive/MyDrive/thesis data/File_data/Evaluation-of-a-modular-construction-system-in-accorda_2023_Journal-of-Build.pdf', 'page': 2}),\n",
       " Document(page_content='perature before the renovation is below 22 /C176C (Danish conditions)\\nit is likely to increase after the renovation and expected savings\\ncould be adjusted.2. Methods\\nAs mentioned earlier, the original goal of the renovation was to\\nmeet the requirements given in the German Passivhaus standard\\n[10]. The Passivhaus standard is normally proven through theoret-\\nical calculations using the Passive House Planning Package [17],\\nhowever for the purpose of this paper the requirements of the\\nstandard are instead compared to the measured normalized energy\\nconsumption for the building. The methods used are described in\\nthe following subsections.\\n2.1. Passivhaus requirements\\nThe renovation was carried out in 2014, which means that the\\nPassivhaus requirements that were in effect at the time are used', metadata={'source': '/content/gdrive/MyDrive/thesis data/File_data/Passive-house-renovation-of-a-block-of-flats---Measured-per_2022_Energy-and-.pdf', 'page': 1}),\n",
       " Document(page_content='cResearch Group Building Construction and Refurbishment (Sicor), Department of Architectural, Civil and Aeronautical Buildings and Structures, \\nHigher Technical University College of Architecture, University of A Coru ˜na, Campus Zapateira, 15008, A Coru ˜na, Spain   \\nARTICLE INFO  \\nKeywords: \\nNearly-zero energy building \\nBuilding component certification \\nLiner tray system \\nPassivhaus criteria \\nHygrothermal performance ABSTRACT  \\nPassivhaus is an energy performance standard for buildings that has been deeply analyzed in the \\ncurrent scientific literature. However, the study of the Passivhaus standard for components and \\nthe use of certified components in the construction of new nearly-zero energy buildings have not', metadata={'source': '/content/gdrive/MyDrive/thesis data/File_data/Evaluation-of-a-modular-construction-system-in-accorda_2023_Journal-of-Build.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search('passivhaus') # default k=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom prompt for the llm \n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "Answer in the same language the question was asked.\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template,\n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever i.e. our vectordb\n",
    "retriever = vectordb.as_retriever(search_kwargs = {\"k\": Config.k, \"search_type\" : \"similarity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaCpp(client=<llama_cpp.llama.Llama object at 0x2a1924df0>, model_path='/Users/bivasbisht/Thesis/llama-2-7b-chat.Q5_K_M.gguf', n_ctx=2048, n_batch=512, n_gpu_layers=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm  # just trying to check if the model is loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "llm,\n",
    "chain_type = \"stuff\",\n",
    "retriever=retriever,\n",
    "chain_type_kwargs={\"prompt\": PROMPT},\n",
    "return_source_documents = True,\n",
    "verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the layout of the generated text from the llm , formatting the llm response\n",
    "\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=700):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
    "\n",
    "    sources_used = ' \\n'.join(\n",
    "        [\n",
    "            source.metadata['source'].split('/')[-1][:-4] + ' - page: ' + str(source.metadata['page'])\n",
    "            for source in llm_response['source_documents']\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ans = ans + '\\n\\nSources: \\n' + sources_used\n",
    "    return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for llm response\n",
    "def llm_ans(query):\n",
    "    start = time.time()\n",
    "    \n",
    "    llm_response = qa_chain.invoke(query)\n",
    "    ans = process_llm_response(llm_response)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    time_elapsed = int(round(end - start, 0))\n",
    "    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n",
    "    return ans + time_elapsed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to C. Maduta et al., nZEB stands for Nearly Zero Energy Buildings, which is a building that requires little or no energy for heating, cooling, and powering its systems.\n",
      "\n",
      "Sources: \n",
      "Towards-climate-neutrality-within-the-European-Union--Assessme_2023_Energy-a - page: 12 \n",
      "Overview-and-future-challenges-of-nearly-zero-energy-build_2022_Energy - page: 11 \n",
      "From-nearly-zero-energy-buildings--NZEB--to-positive-energy_2020_Development - page: 3\n",
      "\n",
      "Time elapsed: 491 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10211.27 ms\n",
      "llama_print_timings:      sample time =      17.45 ms /    45 runs   (    0.39 ms per token,  2579.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22832.92 ms /   351 tokens (   65.05 ms per token,    15.37 tokens per second)\n",
      "llama_print_timings:        eval time =  467220.73 ms /    44 runs   (10618.65 ms per token,     0.09 tokens per second)\n",
      "llama_print_timings:       total time =  490360.52 ms /   395 tokens\n"
     ]
    }
   ],
   "source": [
    "query = \"What is nzeb?\"\n",
    "print(llm_ans(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo : implement in modular programming approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
