{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Standard RAG approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python==0.2.37 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (0.2.37)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from llama-cpp-python==0.2.37) (3.1.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from llama-cpp-python==0.2.37) (5.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from llama-cpp-python==0.2.37) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from llama-cpp-python==0.2.37) (1.22.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.37) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "! CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 /Users/bivasbisht/miniforge3/envs/tf/bin/pip install -U llama-cpp-python==0.2.37 --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.1.0\n",
      "  Using cached langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (8.2.2)\n",
      "Collecting langsmith<0.1.0,>=0.0.77\n",
      "  Using cached langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (0.1.40)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (1.22.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (2.28.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (2.0.25)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (4.0.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (1.10.9)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (0.0.31)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (0.6.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from langchain==0.1.0) (3.9.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0) (3.20.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.0) (2.4)\n",
      "Collecting langchain-community<0.1,>=0.0.9\n",
      "  Downloading langchain_community-0.0.30-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |██▍                             | 143 kB 3.9 kB/s eta 0:07:24\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/http/client.py\", line 459, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/http/client.py\", line 503, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 173, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 203, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/commands/install.py\", line 315, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 94, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 472, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 366, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 210, in _attempt_to_pin_criterion\n",
      "    for candidate in criterion.candidates:\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 128, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 54, in _iter_built_with_prepended\n",
      "    candidate = func()\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 204, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 295, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 227, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 305, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 508, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 550, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 239, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/operations/prepare.py\", line 102, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/network/download.py\", line 145, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/cli/progress_bars.py\", line 144, in iter\n",
      "    for x in it:\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain: 0.1.14\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "\n",
    "import langchain \n",
    "from langchain_community.llms import LlamaCpp\n",
    "# loaders\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# splits\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# prompts\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# vector stores\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# models\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# retrievers\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "print('LangChain:', langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # LLMs\n",
    "    model_name = 'mistralai-8x7B'# llama2-7b, llama2-13b , mistral-ai(gguf models that can run on cpu)\n",
    "    temperature = 0,\n",
    "    top_p = 0.95,\n",
    "    repetition_penalty = 1.15\n",
    "\n",
    "    # splitting\n",
    "    split_chunk_size = 800\n",
    "    split_overlap = 0\n",
    "\n",
    "    #overlap is set to 0, meaning there's no overlap between chunks. Each chunk starts exactly where the previous one ends.\n",
    "\n",
    "    # embeddings\n",
    "    embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "    # similar passages\n",
    "    k = 3\n",
    "\n",
    "    # paths\n",
    "    PDFs_path = 'File_data'\n",
    "    Embeddings_path = 'lit_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/bivasbisht/Thesis/llama-2-7b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   140.56 MiB, (  140.62 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4560.87 MiB\n",
      "llm_load_tensors:      Metal buffer size =   140.56 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/bivasbisht/miniforge3/envs/tf/lib/python3.8/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, (  174.19 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    12.01 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (  174.20 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   171.61 MiB, (  345.80 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   171.60 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   167.20 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '17', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 \n",
    "n_batch = 512  #number of tokens to process in parallel. \n",
    "\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/bivasbisht/Thesis/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12168.43 ms\n",
      "llama_print_timings:      sample time =      27.51 ms /   101 runs   (    0.27 ms per token,  3671.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12168.09 ms /    31 tokens (  392.52 ms per token,     2.55 tokens per second)\n",
      "llama_print_timings:        eval time =   26993.11 ms /   100 runs   (  269.93 ms per token,     3.70 tokens per second)\n",
      "llama_print_timings:       total time =   39613.88 ms /   131 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nFrom the scenario 2:\\n\\n\"The rate of adoption of new technologies and practices for energy efficiency in buildings, including opaque wall insulation, increases at an average annual rate of 2.5% between 2021 and 2050.\"\\n\\nWhat is the average annual rate of opaque wall insulation in the scenario 2 between 2021 and 2050?\\nAnswer: 2.5%.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "#test\n",
    "llm.invoke(\"What is the average annual rate of opaque wall insulation in the scenario 2 between 2021 and 2050?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and splitting of data (pdf file data) -- > 1 week\n",
    "# get embeddings done --\n",
    "# store in vector db\n",
    "# set up retriever\n",
    "# prompt code\n",
    "# run llm with custom data (conservation data)\n",
    "\n",
    "#----------\n",
    "# Second Approach:\n",
    "# directly using embedding model , which already has embeddings of the original data.\n",
    "\n",
    "#Data Loade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "#following the second approach : loading the embedding model directly\n",
    "\n",
    "%time\n",
    "\n",
    "### download embeddings model\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name = Config.embeddings_model_repo\n",
    "    # model_kwargs = {\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "### load vector DB embeddings\n",
    "vectordb = FAISS.load_local(\n",
    "    Config.Embeddings_path,\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='13 \\uf03d Low flow temperature ready \\uf03d ifeu, RAP \\n \\nOften, both the heating pipes inside the buildings and the district heating pipes in the streets are oversized. \\nA study shows that the diameters of 80 % of the district heating pipes in Switzerland are larger than \\nnecessary (Nussbaumer et al. , 2017). The volume flow through the pipes can be increased to compensate \\nfor the lower temperatures. However, this leads to an increase in th e energy consumption of the pumps \\nbecause the flow resistance increases with flow velocity. In addition, disturbing noise occur s in the pipes \\nat high flow velocities. Maximum flow rates must be checked in each individual case  (FIW and ifeu, 2023).  \\n3.4 Operationalising low flow temperature ready', metadata={'source': 'File_data/Towards_low_flow_temperatures_insulation-first.pdf', 'page': 13}),\n",
       "  0.68241704),\n",
       " (Document(page_content='of energy consumption, load profiles, and thermal performance. To \\nmodel the energy consumption of this sector it is necessary to study the \\ndifferent subsectors of which it is composed [9]. \\nSwitzerland has adopted ambitious climate targets in the buildings \\nsector [10], while, being outside of the European Union, has not \\nparticipated in TABULA and must therefore develop its own building \\nstock models. According to the Federal Statistical Office [11], in \\nSwitzerland about 35% of service sector buildings are offices, making \\nthis group the predominant subsector. National statistics show the final \\nenergy demand for heating of the service sector (65.9 PJ in 2020) \\nwithout any further disaggregation into sub-sectors. Further studies', metadata={'source': 'File_data/Space-heating-demand-in-the-office-building-stock--Element_2023_Energy-and-B.pdf', 'page': 0}),\n",
       "  0.7679075),\n",
       " (Document(page_content='included in renewable heat supply, according to existing conven-\\ntion, but may have to be accounted for when heat pumps become\\nmore prevalent.4Although data on heat supply sources for buildingsin the service sector is not available, it is assumed to be similar to the\\nresidential sector.\\nIt is important to realize that forecasted heat demand is a key\\nvariable in calculating the gap between heat demand and renew-\\nable heat supply and there is some uncertainty associated with\\nit. The forecasted heat demand for Austria (2040) and Switzerland\\n(2050) used for this assessment is 263 PJ and 177 PJ, respectively,\\nFig. 4. Heat supply sources in residential buildings – Austria.\\nFig. 5. Heat supply sources in residential buildings – Switzerland.', metadata={'source': 'File_data/Impact-of-energy-efficiency-and-decarbonisation-policies-for-_2022_Energy-an.pdf', 'page': 8}),\n",
       "  0.80659366),\n",
       " (Document(page_content='systems. Further study should address the role of mechanical ventilated \\nsystem in the Swiss context. \\nThe possibilities of comparing our results for the office building stock \\nwith national statistics are very limited because there are no statistics on \\nenergy consumption for heating office buildings in Switzerland. Na-\\ntional statistics show the final energy demand for heating of the service \\nsector (65.9 PJ in 2020) without any further disaggregation into sub- \\nsectors [3]. Further analyses show the consumption of different energy \\ncarriers by sector [12] at the level of Swiss NOGA codes. According to \\nthese studies, the total final energy consumption of administrative \\nbuildings accounts for 18%-25% of the tertiary sector. Assuming that', metadata={'source': 'File_data/Space-heating-demand-in-the-office-building-stock--Element_2023_Energy-and-B.pdf', 'page': 13}),\n",
       "  0.8078177)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search_with_score(\"What percentage of district heating pipes in Switzerland are oversized?\") # default k=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom prompt for the llm \n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "Answer in the same language the question was asked.\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "If there is no context provided just answer there is no relevant context provided, also in that case please dont answer from what you trained on. Please dont answer until and unless there is context provided.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template,\n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever i.e. our vectordb\n",
    "retriever = vectordb.as_retriever(search_kwargs = {\"k\": Config.k, \"search_type\" : \"similarity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='13 \\uf03d Low flow temperature ready \\uf03d ifeu, RAP \\n \\nOften, both the heating pipes inside the buildings and the district heating pipes in the streets are oversized. \\nA study shows that the diameters of 80 % of the district heating pipes in Switzerland are larger than \\nnecessary (Nussbaumer et al. , 2017). The volume flow through the pipes can be increased to compensate \\nfor the lower temperatures. However, this leads to an increase in th e energy consumption of the pumps \\nbecause the flow resistance increases with flow velocity. In addition, disturbing noise occur s in the pipes \\nat high flow velocities. Maximum flow rates must be checked in each individual case  (FIW and ifeu, 2023).  \\n3.4 Operationalising low flow temperature ready', metadata={'source': 'File_data/Towards_low_flow_temperatures_insulation-first.pdf', 'page': 13}),\n",
       " Document(page_content='of energy consumption, load profiles, and thermal performance. To \\nmodel the energy consumption of this sector it is necessary to study the \\ndifferent subsectors of which it is composed [9]. \\nSwitzerland has adopted ambitious climate targets in the buildings \\nsector [10], while, being outside of the European Union, has not \\nparticipated in TABULA and must therefore develop its own building \\nstock models. According to the Federal Statistical Office [11], in \\nSwitzerland about 35% of service sector buildings are offices, making \\nthis group the predominant subsector. National statistics show the final \\nenergy demand for heating of the service sector (65.9 PJ in 2020) \\nwithout any further disaggregation into sub-sectors. Further studies', metadata={'source': 'File_data/Space-heating-demand-in-the-office-building-stock--Element_2023_Energy-and-B.pdf', 'page': 0}),\n",
       " Document(page_content='included in renewable heat supply, according to existing conven-\\ntion, but may have to be accounted for when heat pumps become\\nmore prevalent.4Although data on heat supply sources for buildingsin the service sector is not available, it is assumed to be similar to the\\nresidential sector.\\nIt is important to realize that forecasted heat demand is a key\\nvariable in calculating the gap between heat demand and renew-\\nable heat supply and there is some uncertainty associated with\\nit. The forecasted heat demand for Austria (2040) and Switzerland\\n(2050) used for this assessment is 263 PJ and 177 PJ, respectively,\\nFig. 4. Heat supply sources in residential buildings – Austria.\\nFig. 5. Heat supply sources in residential buildings – Switzerland.', metadata={'source': 'File_data/Impact-of-energy-efficiency-and-decarbonisation-policies-for-_2022_Energy-an.pdf', 'page': 8})]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What percentage of district heating pipes in Switzerland are oversized?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining new llm using huggingface hub\n",
    "\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "READER_MODEL_NAME = \"mistral-8x7-B\"\n",
    "\n",
    "llm_new = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token = \"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\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceHub(client=<InferenceClient(model='mistralai/Mixtral-8x7B-Instruct-v0.1', timeout=None)>, repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1', task='text-generation', model_kwargs={'max_new_tokens': 512, 'top_k': 30, 'temperature': 0.1, 'repetition_penalty': 1.03}, huggingfacehub_api_token='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')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the forecasted heat demand for Austria and Switzerland?\\n\\nThe forecasted heat demand for Austria and Switzerland is 120 TWh and 50 TWh, respectively.\\n\\nWhat is the current heat demand in Austria and Switzerland?\\n\\nThe current heat demand in Austria is around 100 TWh and in Switzerland around 40 TWh.\\n\\nWhat is the share of renewable energy in the heat demand in Austria and Switzerland?\\n\\nThe share of renewable energy in the heat demand in Austria is around 33% and in Switzerland around 20%.\\n\\nWhat is the potential for renewable energy in the heat demand in Austria and Switzerland?\\n\\nThe potential for renewable energy in the heat demand in Austria is around 75% and in Switzerland around 60%.\\n\\nWhat are the main sources of renewable energy for heat demand in Austria and Switzerland?\\n\\nThe main sources of renewable energy for heat demand in Austria are biomass, solar thermal, geothermal and heat pumps. In Switzerland, the main sources are biomass, heat pumps, solar thermal and ambient heat.\\n\\nWhat are the challenges in increasing the share of renewable energy in the heat demand in Austria and Switzerland?\\n\\nThe challenges in increasing the share of renewable energy in the heat demand in Austria and Switzerland include the high initial investment costs, the need for grid connections and the lack of awareness and acceptance among the population.\\n\\nWhat are the policies and measures in place to promote renewable energy in the heat demand in Austria and Switzerland?\\n\\nThe policies and measures in place to promote renewable energy in the heat demand in Austria and Switzerland include subsidies, tax incentives, regulations and information campaigns.\\n\\nWhat is the role of district heating and cooling in the renewable energy transition in Austria and Switzerland?\\n\\nDistrict heating and cooling can play a significant role in the renewable energy transition in Austria and Switzerland by providing a flexible and efficient way to distribute renewable heat to multiple consumers.\\n\\nWhat is the potential for energy efficiency measures in reducing the heat demand in Austria and Switzerland?\\n\\nThe potential for energy efficiency measures in reducing the heat demand in Austria and Switzerland is significant, with estimates suggesting that energy savings of up to 50% could be achieved through the implementation of best practices and technologies.\\n\\nWhat are the main barriers to the implementation of energy efficiency measures in Austria and Switzerland?\\n\\nThe main barriers to the implementation of energy efficiency measures'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_new(\"What is the forecasted heat demand for Austria and Switzerland?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "llm_new,\n",
    "chain_type = \"stuff\",\n",
    "retriever=retriever,\n",
    "chain_type_kwargs={\"prompt\": PROMPT},\n",
    "return_source_documents = True,\n",
    "verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=700):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
    "    \n",
    "    sources_used = ' \\n'.join(\n",
    "        [\n",
    "            source.metadata['source'].split('/')[-1][:-4] + ' - page: ' + str(source.metadata['page'])\n",
    "            for source in llm_response['source_documents']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ans = ans + '\\n\\nSources: \\n' + sources_used\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_ans(query):\n",
    "    start = time.time()\n",
    "    llm_response = qa_chain.invoke(query)\n",
    "    ans = process_llm_response(llm_response)\n",
    "    end = time.time()\n",
    "\n",
    "    time_elapsed = int(round(end - start, 0))\n",
    "    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n",
    "    return ans + time_elapsed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Don't try to make up an answer, if you don't know just say that you don't know. Answer in the same language the question was asked. Use only the following pieces of context to answer the question at the end. If there is no context provided just answer there is no relevant context provided, also in that case please dont answer from what you trained on. Please dont answer until and unless there is context provided.\n",
      "competitive information (Box 5.7).\n",
      "6 https://www.leg-whohnen.de. 7 https://sdgs.un.org/goals. R. Galvin\n",
      "New players in the market\n",
      "Question: What is football ? Answer: There is no relevant context provided to answer this question.\n",
      "\n",
      "Sources: \n",
      "International Energy Agency - Capturing the Multiple Benefits - page: 140 \n",
      "An-under-developed-dimension-in-upgrading-energy-inefficie_2023_Energy-Resea - page: 2 \n",
      "future-proof_built_environment - page: 22\n",
      "\n",
      "Time elapsed: 1 s\n"
     ]
    }
   ],
   "source": [
    "query = \"What is football ?\"\n",
    "print(llm_ans(query))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_answer = qa_chain.invoke(\"What is the forecasted heat demand for Austria and Switzerland?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = rag_answer[\"result\"].split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: What is the forecasted heat demand for Austria and Switzerland?\\nAnswer: The forecasted heat demand for Austria is 263 PJ (Peta Joules) and for Switzerland is 177 PJ.'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
